{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Comp_Vision_Task_4_Students.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PrGQvDKsJVgK"},"source":["В данном блокноте мы рассмотрим несколько вариантов отслеживания объектов на видео.\r\n","\r\n","Начнем с метода Optical Flow, который, по своей сути, сводится к трекингу всех пикселей на видео. Обычно этот метод визуализируют либо непосредственно векторами, либо цветом (каждый цвет кодирует определенный вектор).\r\n","\r\n","Для начала загрузим необходимые библиотеки.\r\n"]},{"cell_type":"code","metadata":{"id":"Ti-Ms8OfJyFv"},"source":["import numpy as np\r\n","import cv2\r\n","from google.colab.patches import cv2_imshow\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xd91njdGznUP"},"source":["Теперь напишем функцию, которая будет полезна для визуализации векторами. На основании полученных данных, она на исходном кадре будет рисовать вектор (наше видео будет обрабатываться покадрово)."]},{"cell_type":"code","metadata":{"id":"5cgTJSBYKM9V"},"source":["def draw_flow(img, flow):\r\n","  step=30\r\n","  h, w = img.shape[:2]\r\n","  y, x = np.mgrid[step / 2:h:step, step / 2:w:step].reshape(2, -1).astype(int)\r\n","  fx, fy = flow[y, x].T\r\n","  lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\r\n","  lines = np.int32(lines + 0.5)\r\n","  image = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\r\n","  for (x1, y1), (x2, y2) in lines:\r\n","    cv2.arrowedLine(image, (x1, y1), (x2, y2), (255, 0, 0), 1, cv2.LINE_AA, 0, 0.3)\r\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WOzQW3Uf0Kir"},"source":["Считаем наш видео файл средствами библиотеки OpenCV. Данный метод предполагает работу с черно-белыми изображениями. Сразу же создадим копию первого кадра нужного размера и закрасим её черным (именно на ней мы увидим визуализацию цветом)."]},{"cell_type":"code","metadata":{"id":"uycmXvVu1QLL"},"source":["cap = cv2.VideoCapture(\"Airport _f.mp4\")\r\n","ret, frame1 = cap.read()\r\n","prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\r\n","hsv = np.zeros_like(frame1)\r\n","hsv[...,1] = 255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-XlKhCO1QpZ"},"source":["К сожалению, данная среда разработки не позволяет демонстрировать видео-файлы. Поэтому мы будем покадрово записывать наши результаты в файл, который в дальнейшем можно посмотреть на любом плеере."]},{"cell_type":"code","metadata":{"id":"FLHYUhzm12bv"},"source":["fourcc = cv2.VideoWriter_fourcc(*'XVID')\r\n","videoWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n","videoHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n","out = cv2.VideoWriter('output_OpticalFlow.avi',fourcc, 20.0,(videoWidth,videoHeight))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqTFMrcvKtYw"},"source":["kk = 0\r\n","while(cap.isOpened()):\r\n","    # Считываем видео кадр за кадром\r\n","    ret, frame2 = cap.read() \r\n","    if ret:\r\n","      next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\r\n","      flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\r\n","      # Получаем векторы и заполняем значениями нашу копию для визуализации цветом\r\n","      mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\r\n","      hsv[...,0] = ang*180/np.pi/2\r\n","      hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\r\n","      bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\r\n","      # Выведем первые 10 кадров на экран с нарисованными векторами\r\n","      if kk < 10:\r\n","        cv2_imshow(draw_flow(next, flow))\r\n","      # Записываем кадр в результирующий файл\r\n","      out.write(bgr)\r\n","      # Переобозначаем кадры - предыдущий становится текущим\r\n","      prvs = next\r\n","      kk += 1\r\n","    else:\r\n","      break\r\n","cap.release()\r\n","out.release()\r\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JsWvW1D8e1P"},"source":["Теперь посмотрим как работает на нашем видео другой детектор — SORT (Simple Online and Realtime Tracking). Для сопоставления траекторий и обнаружения новых объектов, мы будем использовать алгоритм YOLO (You Only Look Once) - венгерский алгоритм, использующий фильтр Калмана для предсказания и корректировки сегментов.\r\n","\r\n","Для начала доустановим необходимые библиотеки."]},{"cell_type":"code","metadata":{"id":"35Ath_5j8fy0"},"source":["!pip install filterpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jab6-Ggj8m-g"},"source":["import random\r\n","\r\n","try:\r\n","  from numba import jit\r\n","except:\r\n","  def jit(func):\r\n","    return func\r\n","\r\n","np.random.seed(0)\r\n","\r\n","from filterpy.kalman import KalmanFilter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kp63qp6v8wl7"},"source":["Воспользуемся открытой для использования уже натренированной моделью Darknet (конфигурация, веса) на СОСО данных. Оставляем все комментарии в исходном виде."]},{"cell_type":"code","metadata":{"id":"Az13Xtyu8zJG"},"source":["def linear_assignment(cost_matrix):\r\n","  try:\r\n","    import lap\r\n","    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\r\n","    return np.array([[y[i],i] for i in x if i >= 0]) #\r\n","  except ImportError:\r\n","    from scipy.optimize import linear_sum_assignment\r\n","    x, y = linear_sum_assignment(cost_matrix)\r\n","    return np.array(list(zip(x, y)))\r\n","\r\n","\r\n","@jit\r\n","def iou(bb_test, bb_gt):\r\n","  \"\"\"\r\n","  Computes IUO between two bboxes in the form [x1,y1,x2,y2]\r\n","  \"\"\"\r\n","  xx1 = np.maximum(bb_test[0], bb_gt[0])\r\n","  yy1 = np.maximum(bb_test[1], bb_gt[1])\r\n","  xx2 = np.minimum(bb_test[2], bb_gt[2])\r\n","  yy2 = np.minimum(bb_test[3], bb_gt[3])\r\n","  w = np.maximum(0., xx2 - xx1)\r\n","  h = np.maximum(0., yy2 - yy1)\r\n","  wh = w * h\r\n","  o = wh / ((bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\r\n","    + (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh)\r\n","  return(o)\r\n","\r\n","\r\n","def convert_bbox_to_z(bbox):\r\n","  \"\"\"\r\n","  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\r\n","    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\r\n","    the aspect ratio\r\n","  \"\"\"\r\n","  w = bbox[2] - bbox[0]\r\n","  h = bbox[3] - bbox[1]\r\n","  x = bbox[0] + w/2.\r\n","  y = bbox[1] + h/2.\r\n","  s = w * h    #scale is just area\r\n","  r = w / float(h)\r\n","  return np.array([x, y, s, r]).reshape((4, 1))\r\n","\r\n","\r\n","def convert_x_to_bbox(x,score=None):\r\n","  \"\"\"\r\n","  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\r\n","    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\r\n","  \"\"\"\r\n","  w = np.sqrt(x[2] * x[3])\r\n","  h = x[2] / w\r\n","  if(score==None):\r\n","    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\r\n","  else:\r\n","    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\r\n","\r\n","\r\n","class KalmanBoxTracker(object):\r\n","  \"\"\"\r\n","  This class represents the internal state of individual tracked objects observed as bbox.\r\n","  \"\"\"\r\n","  count = 0\r\n","  def __init__(self,bbox):\r\n","    \"\"\"\r\n","    Initialises a tracker using initial bounding box.\r\n","    \"\"\"\r\n","    #define constant velocity model\r\n","    self.kf = KalmanFilter(dim_x=7, dim_z=4) \r\n","    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\r\n","    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\r\n","\r\n","    self.kf.R[2:,2:] *= 10.\r\n","    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\r\n","    self.kf.P *= 10.\r\n","    self.kf.Q[-1,-1] *= 0.01\r\n","    self.kf.Q[4:,4:] *= 0.01\r\n","\r\n","    self.kf.x[:4] = convert_bbox_to_z(bbox)\r\n","    self.time_since_update = 0\r\n","    self.id = KalmanBoxTracker.count\r\n","    KalmanBoxTracker.count += 1\r\n","    self.history = []\r\n","    self.hits = 0\r\n","    self.hit_streak = 0\r\n","    self.age = 0\r\n","\r\n","  def update(self,bbox):\r\n","    \"\"\"\r\n","    Updates the state vector with observed bbox.\r\n","    \"\"\"\r\n","    self.time_since_update = 0\r\n","    self.history = []\r\n","    self.hits += 1\r\n","    self.hit_streak += 1\r\n","    self.kf.update(convert_bbox_to_z(bbox))\r\n","\r\n","  def predict(self):\r\n","    \"\"\"\r\n","    Advances the state vector and returns the predicted bounding box estimate.\r\n","    \"\"\"\r\n","    if((self.kf.x[6]+self.kf.x[2])<=0):\r\n","      self.kf.x[6] *= 0.0\r\n","    self.kf.predict()\r\n","    self.age += 1\r\n","    if(self.time_since_update>0):\r\n","      self.hit_streak = 0\r\n","    self.time_since_update += 1\r\n","    self.history.append(convert_x_to_bbox(self.kf.x))\r\n","    return self.history[-1]\r\n","\r\n","  def get_state(self):\r\n","    \"\"\"\r\n","    Returns the current bounding box estimate.\r\n","    \"\"\"\r\n","    return convert_x_to_bbox(self.kf.x)\r\n","\r\n","\r\n","def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\r\n","  \"\"\"\r\n","  Assigns detections to tracked object (both represented as bounding boxes)\r\n","  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\r\n","  \"\"\"\r\n","  if(len(trackers)==0):\r\n","    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\r\n","  iou_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)\r\n","\r\n","  for d,det in enumerate(detections):\r\n","    for t,trk in enumerate(trackers):\r\n","      iou_matrix[d,t] = iou(det,trk)\r\n","\r\n","  if min(iou_matrix.shape) > 0:\r\n","    a = (iou_matrix > iou_threshold).astype(np.int32)\r\n","    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\r\n","        matched_indices = np.stack(np.where(a), axis=1)\r\n","    else:\r\n","      matched_indices = linear_assignment(-iou_matrix)\r\n","  else:\r\n","    matched_indices = np.empty(shape=(0,2))\r\n","\r\n","  unmatched_detections = []\r\n","  for d, det in enumerate(detections):\r\n","    if(d not in matched_indices[:,0]):\r\n","      unmatched_detections.append(d)\r\n","  unmatched_trackers = []\r\n","  for t, trk in enumerate(trackers):\r\n","    if(t not in matched_indices[:,1]):\r\n","      unmatched_trackers.append(t)\r\n","\r\n","  #filter out matched with low IOU\r\n","  matches = []\r\n","  for m in matched_indices:\r\n","    if(iou_matrix[m[0], m[1]]<iou_threshold):\r\n","      unmatched_detections.append(m[0])\r\n","      unmatched_trackers.append(m[1])\r\n","    else:\r\n","      matches.append(m.reshape(1,2))\r\n","  if(len(matches)==0):\r\n","    matches = np.empty((0,2),dtype=int)\r\n","  else:\r\n","    matches = np.concatenate(matches,axis=0)\r\n","\r\n","  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\r\n","\r\n","\r\n","class Sort(object):\r\n","  def __init__(self, max_age=1, min_hits=3):\r\n","    \"\"\"\r\n","    Sets key parameters for SORT\r\n","    \"\"\"\r\n","    self.max_age = max_age\r\n","    self.min_hits = min_hits\r\n","    self.trackers = []\r\n","    self.frame_count = 0\r\n","\r\n","  def update(self, dets=np.empty((0, 5))):\r\n","    \"\"\"\r\n","    Params:\r\n","      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\r\n","    Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections).\r\n","    Returns the a similar array, where the last column is the object ID.\r\n","    NOTE: The number of objects returned may differ from the number of detections provided.\r\n","    \"\"\"\r\n","    self.frame_count += 1\r\n","    # get predicted locations from existing trackers.\r\n","    trks = np.zeros((len(self.trackers), 5))\r\n","    to_del = []\r\n","    ret = []\r\n","    for t, trk in enumerate(trks):\r\n","      pos = self.trackers[t].predict()[0]\r\n","      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\r\n","      if np.any(np.isnan(pos)):\r\n","        to_del.append(t)\r\n","    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\r\n","    for t in reversed(to_del):\r\n","      self.trackers.pop(t)\r\n","    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks)\r\n","\r\n","    # update matched trackers with assigned detections\r\n","    for m in matched:\r\n","      self.trackers[m[1]].update(dets[m[0], :])\r\n","\r\n","    # create and initialise new trackers for unmatched detections\r\n","    for i in unmatched_dets:\r\n","        trk = KalmanBoxTracker(dets[i,:])\r\n","        self.trackers.append(trk)\r\n","    i = len(self.trackers)\r\n","    for trk in reversed(self.trackers):\r\n","        d = trk.get_state()[0]\r\n","        if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\r\n","          ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\r\n","        i -= 1\r\n","        # remove dead tracklet\r\n","        if(trk.time_since_update > self.max_age):\r\n","          self.trackers.pop(i)\r\n","    if(len(ret)>0):\r\n","      return np.concatenate(ret)\r\n","    return np.empty((0,5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0veXDOfJ838q"},"source":["Необходимые веса yolov3.weights можно скачать с сайта https://pjreddie.com/darknet/yolo/."]},{"cell_type":"code","metadata":{"id":"-2E1DU7-86u8"},"source":["import os\r\n","from os.path import exists, join, basename, splitext\r\n","if not exists('yolov3.weights'):\r\n","  !wget -q  https://pjreddie.com/media/files/yolov3.weights  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dg9vK5VD89dk"},"source":["А теперь перейдем непосредственно к детектированию нашего видео. Опять-таки из-за проблем с представлением видео-файлов, мы будем писать результат в отдельный файл."]},{"cell_type":"code","metadata":{"id":"Qu9YXUIb9DT-"},"source":["def load_class_names(namesfile):\r\n","    class_names = []\r\n","    with open(namesfile, 'r') as fp:\r\n","        lines = fp.readlines()\r\n","    for line in lines:\r\n","        line = line.rstrip()\r\n","        class_names.append(line)\r\n","    return class_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"buX28j4u9JV0"},"source":["# Загружаем сеть\r\n","net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\r\n","ln = net.getLayerNames()\r\n","ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n","# Инициализируем трекер\r\n","mot_tracker = Sort()\r\n","# Используем YOLO\r\n","namesfile = 'coco.names'\r\n","class_names = load_class_names(namesfile)\r\n","# Указываем необходимый видео-файл\r\n","cap = cv2.VideoCapture(\"Airport _f.mp4\")\r\n","# Указываем кодеки для результирующего видео\r\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\r\n","videoWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n","videoHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n","# Определяем результирующее видео\r\n","out = cv2.VideoWriter('output_SORT.avi',fourcc, 20.0,(videoWidth,videoHeight))\r\n","# Генерируем будующим прямоугольникам разные цвета\r\n","color_list = []\r\n","for j in range(1000):\r\n","  color_list.append(((int)(random.randrange(255)),(int)(random.randrange(255)),(int)(random.randrange(255))))\r\n","\r\n","kk = 0 \r\n","ret = True\r\n","while ret:\r\n","  ret, img = cap.read()\r\n","  if ret:\r\n","  # Запустим сеть по кадру\r\n","    blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416), swapRB=True, crop=False)\r\n","    h,w,_ = img.shape\r\n","    net.setInput(blob)\r\n","    layerOutputs = net.forward(ln)\r\n","    # Разберём все выходы\r\n","    boxes=[]\r\n","    confidences=[]\r\n","    classIDs=[]\r\n","    for output in layerOutputs:\r\n","      for detection in output:\r\n","        scores = detection[5:]\r\n","        classID = np.argmax(scores)\r\n","        confidence = scores[classID]\r\n","        if confidence > 0.5:\r\n","          box = detection[0:4] * np.array([w, h, w, h])\r\n","          (centerX, centerY, width, height) = box.astype(\"int\")\r\n","          x = int(centerX - (width / 2))\r\n","          y = int(centerY - (height / 2))\r\n","          boxes.append([x, y, int(width), int(height)])\r\n","          confidences.append(float(confidence))\r\n","          classIDs.append(classID)\r\n","    else: \r\n","      break\r\n","      \r\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\r\n","    result_img = np.copy(img)\r\n","    dets = []\r\n","    count_detection=0\r\n","    for j in range(len(idxs)):\r\n","      name = class_names[classIDs[idxs[j][0]]]\r\n","      if name == 'aeroplane':\r\n","        count_detection+=1\r\n","    if count_detection>0:\r\n","      detects = np.zeros((count_detection,5))\r\n","      count=0\r\n","      # Формат, необходимый для трекера\r\n","      for j in range(len(idxs)):\r\n","        b = boxes[idxs[j][0]]\r\n","        name = class_names[classIDs[idxs[j][0]]]\r\n","        if name == 'aeroplane': # указываем необходимую метку для объектов\r\n","          x1 = int(b[0])\r\n","          y1 = int(b[1])\r\n","          x2 = int((b[0] + b[2]))\r\n","          y2 = int((b[1] + b[3]))\r\n","          box = np.array([x1,y1,x2,y2,confidences[idxs[j][0]]])\r\n","          detects[count,:] = box[:]\r\n","          count+=1\r\n","      # Парсим данные трекера\r\n","      if len(detects)!=0:\r\n","        trackers = mot_tracker.update(detects)\r\n","        for d in trackers:\r\n","          result_img = cv2.rectangle(result_img, ((int)(d[0]), (int)(d[1])), ((int)(d[2]), (int)(d[3])), color_list[(int)(d[4])], 2)\r\n","          result_img = cv2.putText(result_img, name + \"-\" + str(int(d[4])), ((int)(d[0]), (int)(d[1]) - 10), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, color_list[(int)(d[4])], 2)\r\n","    \r\n","    if kk % 20 == 0 and kk < 100:\r\n","      cv2_imshow(result_img) # выводим только ограниченное количество кадров \r\n","    out.write(result_img) # пишем кадр в результирующее видео\r\n","    kk += 1\r\n","cap.release()\r\n","out.release()\r\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]}]}